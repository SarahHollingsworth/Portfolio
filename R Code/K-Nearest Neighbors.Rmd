---
title: "K-Nearest Neighbors"
output:
  word_document: default
  html_document: default
date: '2023-06-26'
---

K-Nearest Neighbors

```{r}
library(ISLR)
library(class)
dim(Caravan)

attach(Caravan)

summary(Purchase)

348/5822

standardized.X = scale(Caravan[ , -86])

var(Caravan[, 1])

var(Caravan[ ,2])

var(standardized.X[, 1])

var(standardized.X[ ,2])

test = 1:1000
train.X = standardized.X[-test, ]
test.X = standardized.X[test, ]
train.Y = Purchase[-test]
test.Y = Purchase[test]
set.seed(1)
knn.pred = knn(train.X, test.X, train.Y, k=1)
mean(test.Y!=knn.pred)

mean(test.Y!="No")

table(knn.pred, test.Y)

9 / (68+9)

knn.pred = knn(train.X, test.X, train.Y, k=3)
table(knn.pred, test.Y)

5 / 26

knn.pred = knn(train.X, test.X, train.Y, k=5)
table(knn.pred, test.Y)

4 / 15

glm.fits = glm(Purchase~., data=Caravan, family = binomial, subset=-test)
glm.probs = predict(glm.fits, Caravan[test,], type="response")
glm.pred = rep("No", 1000)
glm.pred[glm.probs>.5] = "Yes"
table(glm.pred, test.Y)

glm.pred = rep("No", 1000)
glm.pred[glm.probs>.25] = "Yes"
table(glm.pred, test.Y)

11 / (22+11)
```

  A. Outline what was discovered when K =1, K=3, and K=5.  
  
  The results of the models show all three do very well when compared to randomly guessing.  When $K = 1$ the model's prediction rate is 11.7%, which is better than the rate if we randomly tried to guess if the person purchased insurance or not, which would be correct only 6% of the time.  The success rate is even higher with $K = 3$ at 19.2% and $K = 5$ at 26.7%, this shows the models accurately predicts when a person will purchase insurance at 3 and 5 times the rate of randomly guessing.  Given how well the KNN model performs, this may also indicate there is a non-linear relationship between the predictor and response variables, and may also lead us to assume there is not a linear decision boundary in our data set.  
  
  B. Outline when you would use KNN over Logistic Regression.  
  
  KNN would be used when the shape of our data does not follow a Gaussian, or normal, distribution and when there is not a clear linear decision boundary.  Also, when we are not interested in the model output providing confidence intervals or coefficients, and are only interested in the class labels.  KNN can be used when we have large datasets and are not interested in determining which predictors have more influence.  